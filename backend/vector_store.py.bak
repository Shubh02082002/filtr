import os
import uuid
import time
from typing import List, Dict
import google.generativeai as genai
from pinecone import Pinecone

# Initialize clients
genai.configure(api_key=os.environ["GEMINI_API_KEY"],transport="rest")
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])


def get_index():
    index_name = os.environ.get("PINECONE_INDEX_NAME", "pmsignal")
    return pc.Index(index_name)


def embed_texts(texts: List[str]) -> List[List[float]]:
    """
    Embed a list of texts using Gemini models/text-embedding-004.
    Batches to respect rate limits (15 RPM on free tier).
    """
    embeddings = []
    batch_size = 10  # conservative batch size for free tier

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        for text in batch:
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=text,
                task_type="retrieval_document"
            )
            embeddings.append(result["embedding"])
            time.sleep(0.1)  # small delay to avoid rate limits

    return embeddings


def embed_query(query: str) -> List[float]:
    """Embed a single query string."""
    result = genai.embed_content(
        model="models/text-embedding-004",
        content=query,
        task_type="retrieval_query"
    )
    return result["embedding"]


def store_chunks(chunks: List[Dict], source_file: str, session_id: str):
    """
    Embed and upsert chunks into Pinecone with metadata.
    """
    index = get_index()

    texts = [c["text"] for c in chunks]
    embeddings = embed_texts(texts)

    vectors = []
    for chunk, embedding in zip(chunks, embeddings):
        chunk_id = f"{session_id}_{uuid.uuid4().hex[:8]}"
        vectors.append({
            "id": chunk_id,
            "values": embedding,
            "metadata": {
                "session_id": session_id,
                "source_file": source_file,
                "source_type": chunk.get("source_type", "unknown"),
                "original_text": chunk["text"][:1000],  # Pinecone metadata limit
                "author": chunk.get("author") or "",
                "timestamp": chunk.get("timestamp") or "",
                "issue_type": chunk.get("issue_type") or "",
            }
        })

    # Upsert in batches of 100
    batch_size = 100
    for i in range(0, len(vectors), batch_size):
        index.upsert(vectors=vectors[i:i + batch_size])

    return len(vectors)


def query_index(query: str, session_id: str, top_k: int = 8) -> List[Dict]:
    """
    Embed query and retrieve top-K chunks from Pinecone filtered by session.
    """
    index = get_index()
    query_vector = embed_query(query)

    results = index.query(
        vector=query_vector,
        top_k=top_k,
        filter={"session_id": {"$eq": session_id}},
        include_metadata=True
    )

    chunks = []
    for match in results.get("matches", []):
        meta = match.get("metadata", {})
        chunks.append({
            "score": round(match.get("score", 0), 3),
            "text": meta.get("original_text", ""),
            "source_file": meta.get("source_file", ""),
            "source_type": meta.get("source_type", ""),
            "author": meta.get("author", ""),
            "timestamp": meta.get("timestamp", ""),
            "issue_type": meta.get("issue_type", ""),
        })

    return chunks
